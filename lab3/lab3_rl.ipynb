{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Lab 3: Policy Search\n",
    "\n",
    "## Task3.4: An agent using Reinforcement Learning\n",
    "\n",
    "Code based on a Maze example presented during CI lecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from copy import deepcopy\n",
    "from operator import xor\n",
    "from itertools import accumulate\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from evolution_strategy import *\n",
    "from functools import cache\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The _Nim_ and _Nimply_ classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Nim import Nimply\n",
    "from Nim import Nim\n",
    "\n",
    "# update_nim() is same as nimming()\n",
    "# adding get_state_and_reward(), give_reward() and is_game_over\n",
    "\n",
    "\n",
    "def is_game_over(self):\n",
    "    # check if robot in the final position\n",
    "    return (sum(r for r in self._rows) == 0)\n",
    "\n",
    "\n",
    "def get_reward(self):\n",
    "    # returning state will be just saving the nim\n",
    "    return self.give_reward()\n",
    "\n",
    "\n",
    "def give_reward(self):\n",
    "    # if at end give 0 reward\n",
    "    # if not at end give -1 reward\n",
    "    return -1 * int(not is_game_over(self))\n",
    "\n",
    "\n",
    "Nim.is_game_over = is_game_over\n",
    "Nim.get_reward = get_reward\n",
    "Nim.give_reward = give_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample (and silly) startegies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pure_random from the lecture\n",
    "def pure_random(state: Nim) -> Nimply:\n",
    "    row = random.choice([r for r, c in enumerate(state.rows) if c > 0])\n",
    "    if state.k == None:\n",
    "        num_objects = random.randint(1, state.rows[row])\n",
    "    elif state.rows[row] < state.k:\n",
    "        num_objects = random.randint(1, state.rows[row])\n",
    "    else:\n",
    "        num_objects = random.randint(1, state.k)\n",
    "    return Nimply(row, num_objects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal strategy using nim sum (a bit different implementation than the lecture)\n",
    "def nim_sum(state: Nim) -> int:\n",
    "    *_, result = accumulate(state.rows, xor)\n",
    "    return result\n",
    "\n",
    "\n",
    "def optimal_strategy(state: Nim) -> Nimply:\n",
    "    # retrieve the possible moves\n",
    "    possible_moves = [(r, o) for r, c in enumerate(state.rows)\n",
    "                      for o in range(1, c + 1)]\n",
    "    if state.k != None:\n",
    "        possible_moves = [p for p in possible_moves if p[1] <= state.k]\n",
    "\n",
    "    # check the values of nim_sum after all possible moves\n",
    "    possible_moves_optimal = list()\n",
    "\n",
    "    for move in possible_moves:\n",
    "        temp_state = deepcopy(state)\n",
    "        temp_state.nimming(Nimply(move[0], move[1]))\n",
    "        if nim_sum(temp_state) == 0:\n",
    "            possible_moves_optimal.append(move)\n",
    "\n",
    "    if possible_moves_optimal == []:\n",
    "        chosen_move = random.choice(possible_moves)\n",
    "    else:\n",
    "        chosen_move = random.choice(possible_moves_optimal)\n",
    "\n",
    "    return Nimply(chosen_move[0], chosen_move[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible_moves(state: Nim):\n",
    "    # retrieve the possible moves\n",
    "    possible_moves = [(r, o) for r, c in enumerate(state.rows)\n",
    "                      for o in range(1, c + 1)]\n",
    "    # possible moves if k was implemented\n",
    "    if state.k != None:\n",
    "        possible_moves = [p for p in possible_moves if p[1] <= state.k]\n",
    "\n",
    "    return possible_moves\n",
    "\n",
    "\n",
    "def possible_new_states(state: Nim):\n",
    "    # returns a list of outcome of all possible moves\n",
    "    new_states_lists = []\n",
    "    for p in possible_moves(state):\n",
    "        temp_state = deepcopy(state)\n",
    "        temp_state.nimming(Nimply(p[0], p[1]))\n",
    "        new_states_lists.append(temp_state)\n",
    "    return new_states_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ending_position(state: Nim):\n",
    "    return (sum(r for r in state.rows) == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, state: Nim, alpha=0.15, random_factor=0.2):  # 80% explore, 20% exploit\n",
    "        self.state_history = []  # state, reward\n",
    "        self.state = state\n",
    "        self.alpha = alpha\n",
    "        self.random_factor = random_factor\n",
    "        # self.moves = possible_moves(state)\n",
    "        self.G = {}\n",
    "        self.init_reward(state)\n",
    "\n",
    "    def init_reward(self, state: Nim):\n",
    "        # retrieve the possible moves\n",
    "        possible_moves = [(r, o) for r, c in enumerate(state.rows)\n",
    "                          for o in range(1, c + 1)]\n",
    "        # possible moves if k was implemented\n",
    "        if state.k != None:\n",
    "            possible_moves = [p for p in possible_moves if p[1] <= state.k]\n",
    "\n",
    "        for m in possible_moves:\n",
    "            self.G[m] = np.random.uniform(low=1.0, high=0.1)\n",
    "\n",
    "    def choose_action(self, allowedMoves):\n",
    "        maxG = -10e15\n",
    "        next_move = None\n",
    "        if np.random.random() < self.random_factor:\n",
    "            # if random number below random factor, choose random action\n",
    "            next_move = random.choice(allowedMoves)\n",
    "        else:\n",
    "            # if exploiting, gather all possible actions and choose one with the highest G (reward)\n",
    "            for move in allowedMoves:\n",
    "                if self.G[move] >= maxG:\n",
    "                    next_move = move\n",
    "                    maxG = self.G[move]\n",
    "\n",
    "        return Nimply(next_move[0], next_move[1])\n",
    "\n",
    "    def update_state_history(self, state, reward):\n",
    "        self.state_history.append((deepcopy(state), reward))\n",
    "\n",
    "    def learn(self):\n",
    "        target = 0\n",
    "\n",
    "        for prev, reward in reversed(self.state_history):\n",
    "            self.G[prev] = self.G[prev] + self.alpha * (target - self.G[prev])\n",
    "            target += reward\n",
    "\n",
    "        self.state_history = []\n",
    "\n",
    "        self.random_factor -= 10e-5  # decrease random factor each episode of play\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversimplified match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:status: Initial board  -> <1 3 5>\n",
      "DEBUG:root:status: After player 0 -> <1 0 5>\n",
      "DEBUG:root:status: After player 1 -> <1 0 2>\n",
      "DEBUG:root:status: After player 0 -> <0 0 2>\n",
      "DEBUG:root:status: After player 1 -> <0 0 1>\n",
      "DEBUG:root:status: After player 0 -> <0 0 0>\n",
      "INFO:root:status: Player 0 won!\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "eval_strategy = pure_random  # player 0\n",
    "\n",
    "nim = Nim(3)\n",
    "robot = Agent(nim, alpha=0.1, random_factor=0.4)\n",
    "\n",
    "logging.debug(f\"status: Initial board  -> {nim}\")\n",
    "player = 0\n",
    "while nim:\n",
    "    if player == 0:\n",
    "        ply = eval_strategy(nim)\n",
    "        nim.nimming(ply)\n",
    "        logging.debug(f\"status: After player {player} -> {nim}\")\n",
    "    else:\n",
    "        # current state is just nim\n",
    "        # choose an action (explore or exploit)\n",
    "        action = robot.choose_action(possible_moves(nim))\n",
    "        nim.nimming(action)  # update the maze according to the action\n",
    "        reward = nim.get_reward()  # get the new state and reward\n",
    "        # update the robot memory with state and reward\n",
    "        robot.update_state_history((action[0], action[1]), reward)\n",
    "        logging.debug(f\"status: After player {player} -> {nim}\")\n",
    "    player = 1 - player\n",
    "winner = 1 - player\n",
    "logging.info(f\"status: Player {winner} won!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# games are ran NUM_MATCHES times to check the average result\n",
    "# who_starts = 0 - recurrent learning starts, who_starts = 1 - recurrent learning goes second\n",
    "def evaluate(NUM_LEARNING_STEPS: int, NUM_MATCHES: int, NIM_SIZE: int, eval_strategy: Callable, who_starts: int, k=None) -> float:\n",
    "\n",
    "    nim = Nim(NIM_SIZE, k)\n",
    "    robot = Agent(nim, alpha=0.1, random_factor=0.2)\n",
    "\n",
    "    for i in range(NUM_LEARNING_STEPS):\n",
    "        won = 0\n",
    "\n",
    "        for m in range(NUM_MATCHES):\n",
    "            nim = Nim(NIM_SIZE, k)\n",
    "            player = 0\n",
    "            while nim:\n",
    "                if player == who_starts:\n",
    "                    # current state is just nim\n",
    "                    # choose an action (explore or exploit)\n",
    "                    action = robot.choose_action(possible_moves(nim))\n",
    "                    # update the maze according to the action\n",
    "                    nim.nimming(action)\n",
    "                    reward = nim.get_reward()  # get the new state and reward\n",
    "                    # update the robot memory with state and reward\n",
    "                    robot.update_state_history((action[0], action[1]), reward)\n",
    "                else:\n",
    "                    ply = eval_strategy(nim)\n",
    "                    nim.nimming(ply)\n",
    "                player = 1 - player\n",
    "            robot.learn()\n",
    "            if player == who_starts:\n",
    "                won += 1\n",
    "        print(\"After\", (i + 1)*NUM_MATCHES, \"win rate:\", won / NUM_MATCHES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 100 win rate: 0.5\n",
      "After 200 win rate: 0.34\n",
      "After 300 win rate: 0.45\n",
      "After 400 win rate: 0.4\n",
      "After 500 win rate: 0.43\n",
      "After 600 win rate: 0.38\n",
      "After 700 win rate: 0.41\n",
      "After 800 win rate: 0.36\n",
      "After 900 win rate: 0.39\n",
      "After 1000 win rate: 0.32\n",
      "After 1100 win rate: 0.34\n",
      "After 1200 win rate: 0.32\n",
      "After 1300 win rate: 0.33\n",
      "After 1400 win rate: 0.39\n",
      "After 1500 win rate: 0.32\n",
      "After 1600 win rate: 0.28\n",
      "After 1700 win rate: 0.39\n",
      "After 1800 win rate: 0.46\n",
      "After 1900 win rate: 0.41\n",
      "After 2000 win rate: 0.32\n",
      "After 2100 win rate: 0.28\n",
      "After 2200 win rate: 0.25\n",
      "After 2300 win rate: 0.29\n",
      "After 2400 win rate: 0.3\n",
      "After 2500 win rate: 0.23\n",
      "After 2600 win rate: 0.37\n",
      "After 2700 win rate: 0.37\n",
      "After 2800 win rate: 0.35\n",
      "After 2900 win rate: 0.31\n",
      "After 3000 win rate: 0.3\n"
     ]
    }
   ],
   "source": [
    "evaluate(30, 100, 3, pure_random, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My algorithm seems to learn how not to play Nim, maybe I messed up the evaluation, maybe the algorithm is wrong. \n",
    "\n",
    "If you see the mistake please let me know champ, otherwise just write in the review that the solution for RL is wrong.\n",
    "\n",
    "At this point I'm giving up cause it's been too much Nim for the last two weeks and I don't know what's wrong and what's right anymore :(\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
